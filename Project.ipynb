{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "8uKHldHqkL2s",
        "HyCd-L1aoRnH",
        "PnZnEjkI6J7N",
        "xF6MjlZo6Z7t",
        "inTL5-4c71en",
        "9VvHyPhy8CWS",
        "XxzCmRiU_jo1",
        "VRtJpOgW-2fJ",
        "YHfs5Kki_ABx",
        "Rsgc3xQpTQU_",
        "jl6ZwsZIimmY",
        "HDzfRnEK95jC"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56hcCCnhjiLB"
      },
      "source": [
        "**CODE BY: NIYATI SINHA**\r\n",
        "\r\n",
        "**ROLL NO: 1805501**\r\n",
        "\r\n",
        "**DARTH VANDER**\r\n",
        "\r\n",
        "######In this notebook where ever I did some mistake or anything, mosty I have just commented that part of code rather than removing it.\r\n",
        "\r\n",
        "######Please read the headings & comments to not what is being done by that block of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uvpmO87jqMJ"
      },
      "source": [
        "#**Importing Libraries and Data-Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5p8DzDuLjrec",
        "outputId": "e87328a5-d7a2-4b37-f1c0-bc02889765c0"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oc1yqm8HjzUX"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "import random\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "import datetime\r\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nA8Z-zQHkB1d"
      },
      "source": [
        "#**Overview Of the DATA SET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1TJrS9lkHUZ"
      },
      "source": [
        "\"\"\" *****************************Data Dictionary**********************\r\n",
        "\r\n",
        "1. business_code : company code of the account\r\n",
        "2.cust_number : customer number given to all the customers of the Account\r\n",
        "3.name_customer : name of the customer\r\n",
        "4.clear_date : The date on which the customer clears an invoice, or in simple terms, they\r\n",
        "make the full payment\r\n",
        "5.buisness_year :\r\n",
        "6.doc_id : It is also an unique identifier of an invoice is a primary key for\r\n",
        "acct_doc_header table\r\n",
        "7.posting_date :\r\n",
        "8.document_create_date : The date on which the invoice document was created\r\n",
        "#                       * The date on which the document was created\r\n",
        "9.document_create_date.1 :\r\n",
        "# due_in_date : The date on which the customer is expected to clear an invoice\r\n",
        "# invoice_currency : The currency of the invoice amount in the document for the invoice\r\n",
        "# document type : It represents the type of document. eg D1 represents Invoice\r\n",
        "# posting_id key : Indicator to identify whether an AR item is invoice, deduction, credit\r\n",
        "memo based on its value. Apllicable for SAP ERP\r\n",
        "# area_business : Business area in sap is defined as an organisationalarea within the financial\r\n",
        "accounting module\r\n",
        "# total_open_amount : The amount that is yet to be paid for that invoice\r\n",
        "# baseline_create_date : The date on which the Invoice was created\r\n",
        "# cust_payment_terms : Business terms and agreements between customers and accounts on\r\n",
        "discounts and days of payment\r\n",
        "# invoice_id : Unique number assigned when a seller creates an Invoice\r\n",
        "# isOpen : Indicator of whether an invoice is open or closed. isopen = 1, means the invoice is open\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uKHldHqkL2s"
      },
      "source": [
        "#**Read Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSH4YiWTkNYD"
      },
      "source": [
        "#Reading 1805501.csv from My Drive and storing it as a dataframe\r\n",
        "df=pd.read_csv(r'/content/drive/My Drive/Colab Datasets/1805501.csv')\r\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJmrMKt7kVov"
      },
      "source": [
        "**Know your Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhXZNCqkkfkc"
      },
      "source": [
        "# Returns the first 5 rows of the dataframe.\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpZNlpivkjFo"
      },
      "source": [
        "# Returns the last 5 rows of the dataframe.\r\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3AraVAIkmof"
      },
      "source": [
        "#Details of each column of the DataFrame df\r\n",
        "#DataFrame.info() : returns information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\"\"\"\r\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSIKe9xFknjG"
      },
      "source": [
        "#Number of invoices that are open and closed\r\n",
        "#invoice open means payment is yet to be done\r\n",
        "temp = df.groupby('isOpen')['isOpen'].count().to_dict()\r\n",
        "print(f\"No. of closed invoices : {temp[0]}\")\r\n",
        "print(f\"No. of open invoices : {temp[1]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfLTbgusktcm"
      },
      "source": [
        "#Number of invoices have clear_date = null\r\n",
        "#Obviously it should be same as number of invoices that are open, still checking it to avoid any case of anomaly. \r\n",
        "print(f\"Invoices having \\\"clear_date\\\" as Nan : {df.loc[:, 'clear_date'].isna().sum()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyCd-L1aoRnH"
      },
      "source": [
        "#**DATA PREPROCESSING AND SANITY CHECK**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnZnEjkI6J7N"
      },
      "source": [
        "###*Conversion of dates to datetime objects*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuHFqGIcoZvo"
      },
      "source": [
        "\"\"\"\r\n",
        "df['clear_date'] = pd.to_datetime(df['clear_date'], format='%Y%m%d %H:%M:%S')\r\n",
        "df['posting_date'] = pd.to_datetime(df['posting_date'], format='%Y-%m-%d')\r\n",
        "df['document_create_date'] = pd.to_datetime(df['document_create_date'], format='%Y-%m-%d')\r\n",
        "df['document_create_date.1'] = pd.to_datetime(df['document_create_date.1'], format='%Y-%m-%d')\r\n",
        "df['due_in_date'] = pd.to_datetime(df['due_in_date']//1, format='%Y-%m-%d')\r\n",
        "df['baseline_create_date'] = pd.to_datetime(df['baseline_create_date']//1, format='%Y-%m-%d')\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XEXg7jNPttUi"
      },
      "source": [
        "print(df.columns) #This returns tuple \r\n",
        "print(type(df.columns)) #pandas index object\r\n",
        "print(\"\\n*****************************************************************\\n\")\r\n",
        "print(df.columns.values) #This returns list of column labels of the dataFrame\r\n",
        "print(type(df.columns.values)) #numpy array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TZHxYtgvPZs"
      },
      "source": [
        "#column.values method returs an array of index.\r\n",
        "print(pd.Series((df.columns.values)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXT5UUEXs1aM"
      },
      "source": [
        "#Series.str.contains() function is used to test if pattern or regex is contained within a string of a Series or Index.\r\n",
        "#Creating a list of Columns that contain data in their column label\r\n",
        "print(pd.Series(list(df.columns)).loc[lambda x : x.str.contains(\"date\")])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-HqgvYUtRSd"
      },
      "source": [
        "dateFormatString = [\"%Y-%m-%d\", \"%Y-%m-%d\", \"%Y%m%d\", \"%Y%m%d\", \"%Y%m%d\", \"%Y%m%d\"]\r\n",
        "ColumnHeadersList = list(pd.Series(list(df.columns.values)).loc[lambda x : x.str.contains(\"date\")])\r\n",
        "#print(ColumnHeaders)\r\n",
        "#print(type(df['isOpen'])) # <class 'pandas.core.series.Series'>\r\n",
        "index_dateFormatString = 0\r\n",
        "for colName in ColumnHeadersList:\r\n",
        "    df[colName] = pd.to_datetime(df[colName].astype(str), format = dateFormatString[index_dateFormatString]).copy()\r\n",
        "    index_dateFormatString = index_dateFormatString + 1\r\n",
        "\r\n",
        "df.head() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF6MjlZo6Z7t"
      },
      "source": [
        "##*Sanity Check*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnxK4lXV6CdF"
      },
      "source": [
        "#Details of each column of the DataFrame df\r\n",
        "#DataFrame.info() : returns information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\"\"\"\r\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_U-RtTM6o4V"
      },
      "source": [
        "#checking for columns containg null values\r\n",
        "df.isnull().any()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vACYI_Q56v46"
      },
      "source": [
        "#column headers in form of list which contain null values\r\n",
        "df.columns[df.isna().any()].tolist()\r\n",
        "#returns the list of those columns which contails atleast one NaN value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URqYM2cO64Nb"
      },
      "source": [
        "#Finding how many datavalues in column invoice_id is null\r\n",
        "df['invoice_id'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZXOuNO-65rR"
      },
      "source": [
        "#Finding how many datavalues in column area_business is null\r\n",
        "df['area_business'].isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inTL5-4c71en"
      },
      "source": [
        "######*Dealing with column 'area_business'*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sXK3KYx87HHt"
      },
      "source": [
        "df['area_business'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6fJEwzRD7se"
      },
      "source": [
        "*Dropping Area_business*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MUYvPJj7Qal"
      },
      "source": [
        "#As all Values are null then there is no significance of this column -> 'area_business'\r\n",
        "#drop column\r\n",
        "df.pop('area_business')\r\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VvHyPhy8CWS"
      },
      "source": [
        "######*Dealing with null values for column 'invoice_id'*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx3BlB5J8Nbs"
      },
      "source": [
        "#as only 6 entries in 50000 are having invoice_id as null thus we can drop these columns to remove any case of ambiguity\r\n",
        "#df.dropna is used to remove rows and columns with null values, by subset I have mentioned it to drop those rows which have null values in the column invoice_id\r\n",
        "df=df.dropna(subset=['invoice_id'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nvz4oIgL8cUr"
      },
      "source": [
        "#Details of each column of the DataFrame df\r\n",
        "#DataFrame.info() : returns information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\"\"\"\r\n",
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gi8bE5Kp9MEu"
      },
      "source": [
        "#**Train, Test, Validation Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XxzCmRiU_jo1"
      },
      "source": [
        "####*Spliting Train & Test on the basis of clear_date*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA07id1x9Ss-"
      },
      "source": [
        "dfTest = df[df.loc[:, \"clear_date\"].isna() == True] #clear_date column value is null means payment to be made\r\n",
        "dfcopyTest=dfTest.copy\r\n",
        "dfTrain = df[df.loc[:, \"clear_date\"].isna() == False] #clear_date column value is non-null means payment already done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ktQAu2JyLNp"
      },
      "source": [
        "dfcopyTest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRtJpOgW-2fJ"
      },
      "source": [
        "####*Test DataFrame*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RO49CwzG9zLV"
      },
      "source": [
        "#DataFrame.info() : returns information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\"\"\"\r\n",
        "dfTest.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ye5ANghh-mtD"
      },
      "source": [
        "dfTest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0sWvj74-WpL"
      },
      "source": [
        "dfTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHfs5Kki_ABx"
      },
      "source": [
        "####*dfTrain DataFrame*\r\n",
        "\r\n",
        "This will be further split into:\r\n",
        "1.   dfTrain\r\n",
        "2.   dfVal\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36Sin1tM99Pm"
      },
      "source": [
        "#DataFrame.info() : returns information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\"\"\"\r\n",
        "dfTrain.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqCCvchC-r5R"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNQqyYof-crf"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4T2kc2NOAt-k"
      },
      "source": [
        "####*Spliting dfTrain into Train DataSet and Validation DataSet*\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AosX0ThVBQOE"
      },
      "source": [
        "\"\"\"For splitting into train and val2 \r\n",
        "splitting should be done on the basis of date as it will include the trends w.r.t time of occurence and season\"\"\"\r\n",
        "\r\n",
        "\r\n",
        "\"\"\"On the basis of Dataset values it is not possible to clearly differentiate among the columns 'posting_date','document_create_date','document_create_date.1' and 'baseline_create_date'\r\n",
        "so first check that whether they are duplicates or not.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpOcwSvJCzWY"
      },
      "source": [
        "######*Comparing columns 'posting_date', 'document_create_date', 'document_create_date.1' and 'baseline_create_date' are duplicate or not*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSVaYOalDB8Z"
      },
      "source": [
        "#df temp is a temporary DataFrame made out of only four columns of the train DataSet\r\n",
        "dfTemp = dfTrain[[\"document_create_date.1\", \"baseline_create_date\", \"document_create_date\", \"posting_date\"]]\r\n",
        "print(dfTemp)\r\n",
        "dfTemp = dfTemp.T #DataFrame.T is done for transpose\r\n",
        "print(dfTemp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQmyDatTEKUD"
      },
      "source": [
        "\"\"\"DataFrame.duplicated() checks for duplicate values and returns boolean values,\r\n",
        " return false if it is the first occurance and \r\n",
        " return true if it is the duplicate occurance (other than first occurance)\"\"\"\r\n",
        "\r\n",
        "temp = dfTemp.duplicated().to_dict()\r\n",
        "print(temp,end='\\n\\n')\r\n",
        "for key in temp:\r\n",
        "    print(f\"{key:22} : Not Duplicate\") if temp[key] == False else print(f\"{key:22} : Duplicate\")\r\n",
        "\r\n",
        "dfTemp = dfTemp.T # again transposing dfTemp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzmhQGKVKCCv"
      },
      "source": [
        "*Drop Duplicate Column*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrCd9bwXL_Kg"
      },
      "source": [
        "dfTemp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xojJbGKhHCDS"
      },
      "source": [
        "#Now dropping Duplicate Column\r\n",
        "dfTemp.pop('posting_date')\r\n",
        "dfTrain.pop('posting_date')\r\n",
        "dfTest.pop('posting_date')\r\n",
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwmG-6NOJ0k6"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaZ0eE0rMUED"
      },
      "source": [
        "dfTemp.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaneCrLTKWLF"
      },
      "source": [
        "*Check Correlation*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7_ztOnJKf64"
      },
      "source": [
        "\"\"\"To find that according to which DataFrame we should split the train and validation dataset\r\n",
        "checking how related are these three columns : \r\n",
        "1: document_create_date.1, \r\n",
        "2: baseline_create_date, \r\n",
        "3: document_create_date \"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DzyX7_MLNDt"
      },
      "source": [
        "\"\"\"The describe() method is used for calculating some statistical data like percentile, mean and standard deviation\r\n",
        " of the numerical values of the Series or DataFrame. \r\n",
        "It analyzes both numeric and object series and also the DataFrame column sets of mixed data types.\"\"\"\r\n",
        "dfTemp.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGlEnpqcNApQ"
      },
      "source": [
        "#Correlation cannot be found on datetime objects so instead of using date we can use day number by choosing a specific startdate\r\n",
        "#choosing the earliest first date among all three columns\r\n",
        "#To see first date among all three columns refer the cell above where we have used the df.describe() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQB68jlIOII8"
      },
      "source": [
        "startDate= datetime.datetime(2018, 11, 19) #from baseline_create_date column\r\n",
        "print(startDate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98pekJ4VPFRb"
      },
      "source": [
        "dfTemp['dayNumber_document_create_date.1'] = (dfTemp['document_create_date.1'] - startDate).dt.days\r\n",
        "dfTemp['dayNumber_document_create_date'] = (dfTemp['document_create_date'] - startDate).dt.days\r\n",
        "dfTemp['dayNumber_baseline_create_date'] = (dfTemp['baseline_create_date'] - startDate).dt.days\r\n",
        "dfTemp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kf7rinAvQRe7"
      },
      "source": [
        "#Calculating Correlation\r\n",
        "print(\"Correlation Matrix : \")\r\n",
        "dfTemp[[\"dayNumber_document_create_date.1\", \"dayNumber_document_create_date\", \"dayNumber_baseline_create_date\"]].corr()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDN63HZyQrQe"
      },
      "source": [
        "#All correlation Matrix values are approximately 1, thus all three columns are very highly correlated\r\n",
        "#Thus futher spliting into train and vaidation can be done by choosing any one of the three columns\r\n",
        "#1: document_create_date.1, 2: baseline_create_date, 3: document_create_date\r\n",
        "\r\n",
        "#I am choosing document_create_date.1 for further spliting\r\n",
        "\r\n",
        "#splitting data into train and validation in 88% and 12% respectively\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "dfTrain, dfValidation = train_test_split(dfTrain, test_size=0.15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwPV6O-dSYes"
      },
      "source": [
        "*Training DataSet*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz9CLzXBSg1B"
      },
      "source": [
        "dfTrain.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "naf7V3gDSmXi"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FneVzo2oSrm3"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw1qqvSMS51p"
      },
      "source": [
        "#DataFrame.info() : returns information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage.\"\"\"\r\n",
        "dfTrain.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlelMoaQ_Gj0"
      },
      "source": [
        "dfValidation.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsgc3xQpTQU_"
      },
      "source": [
        "####*Resetting index of all DataFrames*\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-P6FMr2Tblz"
      },
      "source": [
        "dfTrain.reset_index(drop = True, inplace = True)\r\n",
        "dfValidation.reset_index(drop = True, inplace = True)\r\n",
        "dfTest.reset_index(drop = True, inplace = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvNaHrhxTrLw"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVe6xgkgTuAo"
      },
      "source": [
        "dfValidation.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qx_VrABwTwn_"
      },
      "source": [
        "dfTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfHlp0tyU5Py"
      },
      "source": [
        "#**EDA**\r\n",
        "\r\n",
        "###**Exploratory data analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1WUV9gCVviu"
      },
      "source": [
        "####*Checking columns for unique elements to know the constant features*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWl5BFDMd2FH"
      },
      "source": [
        "dfTrain.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ILK1_fWVzSZ"
      },
      "source": [
        "dfTrain.business_code.unique()\r\n",
        "#mixed feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkY5X7aDV3-H"
      },
      "source": [
        "dfTrain.cust_number.unique()\r\n",
        "#mixed feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP9j9lwHdHMy"
      },
      "source": [
        "dfTrain.name_customer.unique()\r\n",
        "#string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa9hSLlOV-oy"
      },
      "source": [
        "dfTrain.buisness_year.unique()\r\n",
        "#float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGvjT13XWAcv"
      },
      "source": [
        "dfTrain.doc_id.unique()\r\n",
        "#double"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBjfzvT9WDHs"
      },
      "source": [
        "dfTrain.invoice_currency.unique()\r\n",
        "#currency -> str"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dlxx8XaRexg4"
      },
      "source": [
        "#dfTrain.document type.unique() -> error as label contains space\r\n",
        "df['document type'].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYuwU6PufVMg"
      },
      "source": [
        "dfTrain.total_open_amount.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lbAIdcmfl5E"
      },
      "source": [
        "dfTrain.cust_payment_terms.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQdg01fPfqii"
      },
      "source": [
        "dfTrain.invoice_id.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlQN95pdfz-W"
      },
      "source": [
        "dfTrain.isOpen.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49O1zGIV645"
      },
      "source": [
        "dfTrain.posting_id.unique()\r\n",
        "#float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeddzpdkho0u"
      },
      "source": [
        "#**Feature Engineering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8m4kiYeUb65"
      },
      "source": [
        "####*Handling categorical features*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTiTPP9mXyET"
      },
      "source": [
        "######*Identifying Mixed features*\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qq60aIgpZHsG"
      },
      "source": [
        "dfTrain.info()\r\n",
        "#We have to identify mixed features from columns having Dtype = object as all others are specified of specific Dtype\r\n",
        "\r\n",
        "'''\r\n",
        "Columns to consider for mixed features:\r\n",
        " 0   business_code  \r\n",
        " 1   cust_number \r\n",
        " 2   name_customer   \r\n",
        " 9   invoice_currency  \r\n",
        " 10  document type\r\n",
        " 13  cust_payment_terms    '''\r\n",
        "\r\n",
        " #to check for mixed values check for unique() values in these columns done under the sub-header 'Checking columns for unique elements to know the constant features' of EDA\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4L0b3Wv_X739"
      },
      "source": [
        "#Mixed features means features containing more than one dataType\r\n",
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyMbpOMFVBqn"
      },
      "source": [
        "#by refering to the unique values in the identified columns mentioned in the first comment of this sub heading\r\n",
        "#mixed variable clearly visible: business_code, cust_payment_terms \r\n",
        "#mixed varable not so clearly visible: cust_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jl6ZwsZIimmY"
      },
      "source": [
        "######*business_code*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5LsW-Wnisq2"
      },
      "source": [
        "dfTrain[\"business_code\"].unique()\r\n",
        "#first char part either U or CA\r\n",
        "#second int part"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRkCRllZjN06"
      },
      "source": [
        "def businessCodeDealing(dataf):\r\n",
        "    #seperate string part from business_code and replace U by 0 and CA by 1(for one hot encoding to be implemented)\r\n",
        "\r\n",
        "    dataf.loc[dataf[\"business_code\"].str.contains(\"^U\") == True , \"business_code_str_part\"] = 0\r\n",
        "    dataf.loc[dataf[\"business_code\"].str.contains(\"^CA\") == True , \"business_code_str_part\"] = 1\r\n",
        "\r\n",
        "    dataf[\"business_code\"] = dataf[\"business_code\"].apply(lambda x : int(x[2:]))\r\n",
        "    dataf.rename(columns = {\"business_code\" : \"business_code_int_part\"}, inplace = True)\r\n",
        "    return dataf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LW1IsheSj3YH"
      },
      "source": [
        "dfTrain = businessCodeDealing(dfTrain.copy())\r\n",
        "dfTest = businessCodeDealing(dfTest.copy())\r\n",
        "dfValidation = businessCodeDealing(dfValidation.copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FeJriHpkJr_"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SjTT05C_WVp"
      },
      "source": [
        "dfTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuZ4o2V8_aiH"
      },
      "source": [
        "dfValidation.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsf2yQmc9kdc"
      },
      "source": [
        "One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01aas9gBqKBO"
      },
      "source": [
        "#oneHotEncoding the business_code_string_Part\r\n",
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "encoder = OneHotEncoder(handle_unknown='ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mVbDqAQmqRbK"
      },
      "source": [
        "enc_df = pd.DataFrame(encoder.fit_transform(dfTrain[[\"business_code_str_part\"]]).toarray())\r\n",
        "enc_df = enc_df.rename(columns = {0 : \"business_code_str_part_U\", 1 : \"business_code_str_part_CA\"})\r\n",
        "dfTrain = dfTrain.join(enc_df).drop(columns = [\"business_code_str_part\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZzpKJ-xqibX"
      },
      "source": [
        "def encoding(dataf, encoder):\r\n",
        "    enc_df = pd.DataFrame(encoder.transform(dataf[[\"business_code_str_part\"]]).toarray())\r\n",
        "    enc_df = enc_df.rename(columns = {0 : \"business_code_str_part_U\", 1 : \"business_code_str_part_CA\"})\r\n",
        "    dataf = dataf.join(enc_df).drop(columns = [\"business_code_str_part\"])\r\n",
        "    return dataf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5a7hApxdqtOv"
      },
      "source": [
        "dfValidation = encoding(dfValidation, encoder)\r\n",
        "dfTest = encoding(dfTest, encoder)\r\n",
        "\r\n",
        "#oneHotEncoding the business_code_string_Part Successfull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4R5LqP3rQDr"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-OR1cS_nUv"
      },
      "source": [
        "dfTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oNlwhoU_oBe"
      },
      "source": [
        "dfValidation.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDzfRnEK95jC"
      },
      "source": [
        "######*cust_payment_terms*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-R5cfHab-ALY"
      },
      "source": [
        "dfTrain[\"cust_payment_terms\"].unique()\r\n",
        "#sometimes set of characters (alpha) followed by int\r\n",
        "#sometimes all characters (alpha)\r\n",
        "#sometimes int part followed by set of characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMa7zlNyAKhj"
      },
      "source": [
        "\"\"\"column 'cust_payment_terms' cannot be segregated into integers and alphabetical part\r\n",
        "thus one hot encoding seems to be not possible\r\n",
        "so, encode it directly\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKWOR92PAmnR"
      },
      "source": [
        "#Done in the sub Heading Mean Encoding cust_payment_terms later"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djLSsJ-AAm32"
      },
      "source": [
        "######*invoice_currency*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOvN7bPqAuxB"
      },
      "source": [
        "dfTrain[\"invoice_currency\"].unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxejMSSdA7jZ"
      },
      "source": [
        "#Calculating percentage of train set that uses USD and CAD as currency\r\n",
        "print(f' USD = {(dfTrain[dfTrain[\"invoice_currency\"] == \"USD\"].shape[0] / dfTrain.shape[0])*100} % of total dfTrain')\r\n",
        "print(f' CSD = {100- (dfTrain[dfTrain[\"invoice_currency\"] == \"USD\"].shape[0] / dfTrain.shape[0])*100} % of total dfTrain')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9gN95KZB1Sa"
      },
      "source": [
        "#As the data is split into significant amounts of data thus the lower frequesncy of CSD cannot be ignored\r\n",
        "#It would have been possible to ignore it if it was somewhere arund 1% or even lesser\r\n",
        "print(f' No of data-entries having USD as invoice_currency {dfTrain[dfTrain[\"invoice_currency\"] == \"USD\"].shape[0]}')\r\n",
        "print(f' No of data-entries having CAD as invoice_currency {dfTrain[dfTrain[\"invoice_currency\"] == \"CAD\"].shape[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pu_P4lW2KbDH"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3tPKCmuDIuQ"
      },
      "source": [
        "#As my data shows most of the  transactions are done in USD so using USD as Standard mode of payment\r\n",
        "##2019's Average exchange rate : 1 CAD = 0.753598 USD\r\n",
        "#\tCanadian Dollar to US Dollar exchange rate as on Sunday 31 January 2021\tis 1 CAD = 0.78102 USD\r\n",
        "\r\n",
        "def currencyConverter(dataf):\r\n",
        "    #converting CAD to USD\r\n",
        "\r\n",
        "    #converting total_open_amount to USD wherever invoice_currency is 'CAD'\r\n",
        "    dataf.loc[dataf.loc[:, \"invoice_currency\"] == \"CAD\", \"total_open_amount\"] = dataf.loc[dataf.loc[:, \"invoice_currency\"] == \"CAD\", \"total_open_amount\"].apply(lambda x : 0.78102 * x)\r\n",
        "    \r\n",
        "    #after converting total_open_amount to USD wherever invoice_currency is 'CAD' changing the invoice_currenct to USD\r\n",
        "    dataf.loc[dataf.loc[:, \"invoice_currency\"] == \"CAD\", \"invoice_currency\"] = \"USD\"\r\n",
        "    return dataf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbGcTXKVIDSW"
      },
      "source": [
        "dfTrain = currencyConverter(dfTrain.copy())\r\n",
        "dfTest = currencyConverter(dfTest.copy())\r\n",
        "dfValidation = currencyConverter(dfValidation.copy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMwXZVVSIwFG"
      },
      "source": [
        "dfTrain.invoice_currency.unique(), dfTest.invoice_currency.unique(), dfValidation.invoice_currency.unique()\r\n",
        "#constant feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fq43M19ZIybZ"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-tlTODtjAXQ"
      },
      "source": [
        "######*Mean Encoding cust_payment_terms*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXOwhEHZjFxy"
      },
      "source": [
        "#creating a mapper function to map \"cust_payment_terms\" column's values to respective mean\r\n",
        "temp = dfTrain[[\"cust_payment_terms\", \"total_open_amount\"]]\r\n",
        "meanMapper = temp.groupby(\"cust_payment_terms\")[\"total_open_amount\"].mean().to_dict()\r\n",
        "print(meanMapper)\r\n",
        "meanMapper = {key : round(value, 2) for key, value in meanMapper.items()}\r\n",
        "print(meanMapper)\r\n",
        "\r\n",
        "#meanMapper is the mapper dictionary"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "so-0CDf_jNIr"
      },
      "source": [
        "def meanEncoding(dataf):\r\n",
        "    dataf.loc[:, \"cust_payment_terms\"] = dataf.loc[:, \"cust_payment_terms\"].map(lambda x : meanMapper.get(x, 0))\r\n",
        "    return dataf.copy()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi9iwyImjUHM"
      },
      "source": [
        "dfTrain = meanEncoding(dfTrain)\r\n",
        "dfTest = meanEncoding(dfTest)\r\n",
        "dfValidation = meanEncoding(dfValidation)\r\n",
        "\r\n",
        "#mean encoding the cust_payment_terms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atwwBNvCjSB-"
      },
      "source": [
        "pd.set_option('display.max_columns', None)\r\n",
        "\r\n",
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHHHrbwkkHyL"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UL3WXevvqZj"
      },
      "source": [
        "######*document type*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0K-F3L_wNgz"
      },
      "source": [
        "temp = dfTrain.copy()\r\n",
        "temp[\"document type\"].unique()\r\n",
        "#constant feature as far as train dataset is considered"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPwGUXvL1GcU"
      },
      "source": [
        "#**EDA**  ( continued )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSASpDHH1TP3"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUzUuaEd1efX"
      },
      "source": [
        "temp=dfTrain.copy()\r\n",
        "temp.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjNPXb2g3nNm"
      },
      "source": [
        "temp[\"clear_date_month\"] = temp[\"clear_date\"].dt.month\r\n",
        "temp[\"clear_date_year\"] = temp[\"clear_date\"].dt.year\r\n",
        "temp[\"doc_create_date_month\"] = temp[\"document_create_date.1\"].dt.month\r\n",
        "temp[\"doc_create_date_year\"] = temp[\"document_create_date.1\"].dt.year\r\n",
        "\r\n",
        "temp[\"paid_on_or_before_due_date\"] = np.where(temp.apply(lambda x : x[\"clear_date\"] <= x[\"due_in_date\"], axis = 1) == True, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTCXZAuv5QlI"
      },
      "source": [
        "temp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmz1kVGl6bmN"
      },
      "source": [
        "temp[\"payment_delay_days\"] = temp[\"clear_date\"].sub(temp[\"due_in_date\"]).dt.days.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Loij6Kss6u5f"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(15,12)})\r\n",
        "sns.heatmap(temp.corr(), annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2bhRgf-o61Fb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RR0oQQi7y2i"
      },
      "source": [
        "###*Constant Feature Removal*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Tu8V00O725J"
      },
      "source": [
        "\"\"\"Constant features are the type of features that contain only one value for all the outputs in the dataset.\r\n",
        " Constant features provide no information that can help in classification of the record at hand.\r\n",
        "Therefore, it is advisable to remove all the constant features from the dataset.\"\"\"\r\n",
        "\r\n",
        "#Removing constant features as they don't provide any useful information to a machine learning model to predict a target feature."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXViBH_u88Ld"
      },
      "source": [
        "def constFeatureCheck(dataf):\r\n",
        "    print(\"No. of unique values in each feature :\\n\")\r\n",
        "    constFeatures = []\r\n",
        "    for item in dataf.columns.values:\r\n",
        "        uniqueItems = len(dataf[item].unique())\r\n",
        "        print(f\"{item:22} : {uniqueItems} \")\r\n",
        "        if(uniqueItems == 1):\r\n",
        "            constFeatures.append(item)\r\n",
        "    return constFeatures\r\n",
        "\r\n",
        "constFeaturesDataTrain = constFeatureCheck(dfTrain)\r\n",
        "print(f\"\\nList of constant features in dataTrain: {constFeaturesDataTrain}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_-hwz_79kzT"
      },
      "source": [
        "#### Removing constant columns of train set from all the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNj39jd79pbH"
      },
      "source": [
        "dfTrain = dfTrain.drop(columns = constFeaturesDataTrain).copy()\r\n",
        "dfValidation = dfValidation.drop(columns = constFeaturesDataTrain).copy()\r\n",
        "dfTest = dfTest.drop(columns = constFeaturesDataTrain).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoRKrLhV91h_"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQ1Xg03n-Lm3"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOPgfb3v-yat"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8m8eBL9CAPcm"
      },
      "source": [
        "### *Quasi - Constant Removal*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h4kBDXgTATrl"
      },
      "source": [
        "\"\"\"Quasi-constant features, as the name suggests, are the features that are almost constant. \r\n",
        "In other words, these features have the same values for a very large subset of the output\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw4Y4TpTBMEv"
      },
      "source": [
        "#The features having very small variance values have very negligible use for a machine learning model\r\n",
        "#to predict the target feature\r\n",
        "#thus removing them"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CtSCC56BW9f"
      },
      "source": [
        "def quasiConstFeatureCheck(dataf, threshold = 1.0):\r\n",
        "    print(\"No. of unique values in each feature :\\n\")\r\n",
        "    quasiConstFeatures = []\r\n",
        "    for item in dataf.columns.values:\r\n",
        "        uniqueItems = len(dataf[item].unique())\r\n",
        "        percentage = (uniqueItems / dataf.shape[0]) * 100\r\n",
        "        print(f\"{item:28} : {uniqueItems:5} : {percentage:.2f}%\")\r\n",
        "        if(percentage <= threshold):\r\n",
        "            quasiConstFeatures.append(item)\r\n",
        "    return quasiConstFeatures\r\n",
        "\r\n",
        "#considering those features as quasi-constant which have less than 0.1% of distinct records\r\n",
        "quasiConstFeaturesDataTrain = quasiConstFeatureCheck(dfTrain, 0.1)\r\n",
        "print(f\"\\nList of quasi-constant features in dataTrain:\\n{quasiConstFeaturesDataTrain}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXhb2s_yBjLQ"
      },
      "source": [
        "#Out of these 4 features 'business_code_int_part', 'buisness_year', 'business_code_str_part_U', 'business_code_str_part_CA'\r\n",
        "#only 'buisness_year' seems suitable to be dropped\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjzqxNucF6SO"
      },
      "source": [
        "######Removing Quasi-Constant columns of train set from all the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj9aKhtQGK45"
      },
      "source": [
        "dfTrain = dfTrain.drop(columns = \"buisness_year\").copy()\r\n",
        "dfValidation = dfValidation.drop(columns = \"buisness_year\").copy()\r\n",
        "dfTest = dfTest.drop(columns = \"buisness_year\").copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlhrkW92Go3J"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jcpYfGTHl6r"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXSRHlp3Ho6t"
      },
      "source": [
        "dfTrain.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaJbsjGoKLzZ"
      },
      "source": [
        "###*Checking for duplicate columns*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7hM6WEjKSx9"
      },
      "source": [
        "#From observation: Possible groups of duplicate features can be:\r\n",
        "#1. (\"doc_id\", \"invoice_id\")\r\n",
        "#2. (\"document_create_date.1\", \"baseline_create_date\", \"document_create_date\")\r\n",
        "\"\"\"\r\n",
        "but group 2 has already been checked \r\n",
        "under the heading 'Comparing columns 'posting_date', 'document_create_date', 'document_create_date.1' and 'baseline_create_date' are duplicate or not' \r\n",
        "with \"posting_date\"\r\n",
        "\r\n",
        "So, we need to check only group1\r\n",
        "\r\n",
        "\"\"\"\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Btd0ww9tTNh0"
      },
      "source": [
        "####Group1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thiVo9tELrHb"
      },
      "source": [
        "#Checking for 1st group:\r\n",
        "temp = dfTrain[[\"doc_id\", \"invoice_id\"]]\r\n",
        "#print(temp)\r\n",
        "\r\n",
        "temp = temp.T\r\n",
        "#print(temp)\r\n",
        "\r\n",
        "zz = temp.duplicated().to_dict()\r\n",
        "print(zz)\r\n",
        "\r\n",
        "for key in zz:\r\n",
        "    print(f\"{key:22} : Not Duplicate\") if zz[key] == False else print(f\"{key:22} : Duplicate\")\r\n",
        "\r\n",
        "temp = temp.T # again transposing dfTemp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44ysrI9wRPIf"
      },
      "source": [
        "#####Removing duplicate column invoice_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dX1tYXo3LuDt"
      },
      "source": [
        "dfTrain = dfTrain.drop(columns = \"invoice_id\").copy()\r\n",
        "dfValidation = dfValidation.drop(columns = \"invoice_id\").copy()\r\n",
        "dfTest = dfTest.drop(columns = \"invoice_id\").copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02zzVzzYRqZR"
      },
      "source": [
        "dfTrain.shape, dfValidation.shape, dfTest.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aAtJA5zTbv4"
      },
      "source": [
        "####Group2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qQjELwlTfi0"
      },
      "source": [
        "\"\"\"but group 2 has already been checked \r\n",
        "under the heading 'Comparing columns 'posting_date', 'document_create_date', 'document_create_date.1' and 'baseline_create_date' are duplicate or not' \r\n",
        "with \"posting_date\"\r\n",
        "\r\n",
        "There we have found that all three columns 'document_create_date', 'document_create_date.1' and 'baseline_create_date' are different\r\n",
        "\r\n",
        "And all of them are highly coorelated\r\n",
        "\r\n",
        "And we used column document_create_date.1 for splitting thus we can drop the other two\r\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOAQfRkbYeVF"
      },
      "source": [
        "######Removing highly correlated features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hbW4-2--VHIK"
      },
      "source": [
        "highlyCorrelated = [\"baseline_create_date\", \"document_create_date\"]\r\n",
        "dfTrain = dfTrain.drop(columns = highlyCorrelated).copy()\r\n",
        "dfValidation = dfValidation.drop(columns = highlyCorrelated).copy()\r\n",
        "dfTest = dfTest.drop(columns = highlyCorrelated).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLS8fw1NUo1K"
      },
      "source": [
        "dfTrain.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "103fWyShXxwM"
      },
      "source": [
        "dfTrain.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMkECSJ4YxKe"
      },
      "source": [
        "#**Feature Engineering** ( continued )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UbtON8q7apPN"
      },
      "source": [
        "temp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2dlFCdUY2OP"
      },
      "source": [
        "temp = dfTrain.copy()\r\n",
        "\r\n",
        "#payment_delay_date = clear_date - due_in_date\r\n",
        "#after how many days of the due_in_date the customer made the payment\r\n",
        "temp[\"payment_delay_days\"] = temp[\"clear_date\"].sub(temp[\"due_in_date\"]).dt.days.copy()\r\n",
        "#if payment delay days in negative it means that customer made the payment before the due date\r\n",
        "#Similarly if payment delay days in positive it means that customer made the payment after the due date\r\n",
        "\r\n",
        "\r\n",
        "#payment orig_payment_window_days = due_in_date - document_create_date.1\r\n",
        "#after how many days of the invoice document create date we have the due date\r\n",
        "temp[\"orig_payment_window_days\"] = temp[\"due_in_date\"].sub(temp[\"document_create_date.1\"]).dt.days.copy()\r\n",
        "temp[\"Total_no_days_taken\"]=temp[\"clear_date\"].sub(temp[\"document_create_date.1\"]).dt.days.copy()\r\n",
        "\r\n",
        "\r\n",
        "#dfValidation[\"Total_no_days_taken\"]=dfValidation[\"clear_date\"].sub(dfValidation[\"document_create_date.1\"]).dt.days.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBvE_1HlZ_Qb"
      },
      "source": [
        "temp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5r9KurpaHgi"
      },
      "source": [
        "temp.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbyJ6ea9alos"
      },
      "source": [
        "temp[temp[\"payment_delay_days\"] <= 0].shape, temp[temp[\"payment_delay_days\"] > 0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emJkVyoObnaz"
      },
      "source": [
        "temp[(temp[\"Total_no_days_taken\"] >= 200) | (temp[\"total_open_amount\"] >= 300000)].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4nKgXQKcdwO"
      },
      "source": [
        "temp = temp[(temp[\"Total_no_days_taken\"] < 160) & (temp[\"total_open_amount\"] < 300000)]\r\n",
        "temp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOdrBB6Qcm0O"
      },
      "source": [
        "\"\"\" The different Ageing buckets will be :\r\n",
        "● 0-15 days\r\n",
        "● 16-30 days\r\n",
        "● 31-45 days\r\n",
        "● 46-60 days\r\n",
        "● Greater than 60 days\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gn6WtlfddJjy"
      },
      "source": [
        "bin = [x * 15 for x in range(0,10,1)]\r\n",
        "bin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7dOxPrWdLBb"
      },
      "source": [
        "plt.figure(figsize=(10,6), dpi=80)\r\n",
        "n, bins, patches = plt.hist(temp[\"Total_no_days_taken\"], bins = bin, rwidth = 0.9, facecolor='g', alpha=0.75)\r\n",
        "plt.xlabel(\"Total days taken to pay\")\r\n",
        "plt.ylabel(\"No. of invoices cleared\")\r\n",
        "plt.xticks(bin)\r\n",
        "plt.grid(True)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xFudfSMdaj5"
      },
      "source": [
        "#This means majority of invoices are cleared within 40 days of creation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFle3sMFdkZV"
      },
      "source": [
        "sns.set(rc={'figure.figsize':(12,10)})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0ONM-U8dzv9"
      },
      "source": [
        "sns.heatmap(temp.corr(), annot = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1yw4V00g9yD"
      },
      "source": [
        "temp['create_month'] = pd.DatetimeIndex(temp['document_create_date.1']).month \r\n",
        "temp['create_weekDay'] = pd.DatetimeIndex(temp['document_create_date.1']).weekday \r\n",
        "temp['create_day'] = pd.DatetimeIndex(temp['document_create_date.1']).day\r\n",
        "temp['due_in_month'] = pd.DatetimeIndex(temp['due_in_date']).month \r\n",
        "temp['due_in_weekDay'] = pd.DatetimeIndex(temp['due_in_date']).weekday\r\n",
        "temp.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJX2PFpVixjz"
      },
      "source": [
        "dfValidation['create_month'] = pd.DatetimeIndex(dfValidation['document_create_date.1']).month \r\n",
        "dfValidation['create_weekDay'] = pd.DatetimeIndex(dfValidation['document_create_date.1']).weekday \r\n",
        "dfValidation['create_day'] = pd.DatetimeIndex(dfValidation['document_create_date.1']).day\r\n",
        "dfValidation['due_in_month'] = pd.DatetimeIndex(dfValidation['due_in_date']).month \r\n",
        "dfValidation['due_in_weekDay'] = pd.DatetimeIndex(dfValidation['due_in_date']).weekday\r\n",
        "dfValidation.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tpBwva6pJQp"
      },
      "source": [
        "dfTest['create_month'] = pd.DatetimeIndex(dfTest['document_create_date.1']).month \r\n",
        "dfTest['create_weekDay'] = pd.DatetimeIndex(dfTest['document_create_date.1']).weekday \r\n",
        "dfTest['create_day'] = pd.DatetimeIndex(dfTest['document_create_date.1']).day\r\n",
        "dfTest['due_in_month'] = pd.DatetimeIndex(dfTest['due_in_date']).month \r\n",
        "dfTest['due_in_weekDay'] = pd.DatetimeIndex(dfTest['due_in_date']).weekday\r\n",
        "dfTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAOMaaYzd0Va"
      },
      "source": [
        "temp['payment_delay_days'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLb706WUfUFe"
      },
      "source": [
        "In order to see how is the distribution we can use sb.distplot to plot it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOPXqCx7fU7z"
      },
      "source": [
        "temp.corr().loc['payment_delay_days',:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bdsDLGrfdO3"
      },
      "source": [
        "sns.distplot(temp['payment_delay_days'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPWZJEz-f-9S"
      },
      "source": [
        "plt.figure(figsize=(20,5))\r\n",
        "sns.lineplot(x=temp['document_create_date.1'], y=temp['payment_delay_days'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JB01fqRvfmN9"
      },
      "source": [
        "sns.lineplot(x=temp['payment_delay_days'],y=temp['total_open_amount'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMcUeES5giyD"
      },
      "source": [
        "sns.scatterplot(temp['due_in_weekDay'],temp['payment_delay_days'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2P2Q8v1qgvAi"
      },
      "source": [
        "sns.barplot(x=temp['document_create_date.1'], y=temp['payment_delay_days'])\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nNR7S87h7GG"
      },
      "source": [
        "corr=temp.corr()\r\n",
        "plt.figure(figsize=(20,10))\r\n",
        "sns.heatmap(corr,annot=True)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf11T6pvqJVU"
      },
      "source": [
        "temp.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EppeLc0NinHg"
      },
      "source": [
        "tui_df = temp.drop(columns = [\"cust_number\", \"name_customer\"]).copy()\r\n",
        "tui_df.columns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TQQTVGwIi95Y"
      },
      "source": [
        "tui_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWpICNIyjaAY"
      },
      "source": [
        "#tui_df[\"payment_delay_days\"] = tui_df[\"clear_date\"].sub(tui_df[\"due_in_date\"]).dt.days.copy()\r\n",
        "#tui_df[\"orig_payment_window_days\"] = tui_df[\"due_in_date\"].sub(tui_df[\"document_create_date.1\"]).dt.days.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6VmdNTRjreD"
      },
      "source": [
        "tui_df_Y = tui_df[\"payment_delay_days\"]\r\n",
        "tui_df_X = tui_df.drop(columns = [\"payment_delay_days\", \"clear_date\", \"document_create_date.1\", \"due_in_date\"]).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuflEAZ5kaPz"
      },
      "source": [
        "dfValidation[\"Total_no_days_taken\"]=dfValidation[\"clear_date\"].sub(dfValidation[\"document_create_date.1\"]).dt.days.copy()\r\n",
        "tui_val1_df = dfValidation.drop(columns = [\"cust_number\", \"name_customer\"]).copy()\r\n",
        "\r\n",
        "\r\n",
        "tui_val1_df[\"total_open_amount\"] = tui_val1_df[\"total_open_amount\"].map(lambda x : round(x, 2))\r\n",
        "tui_val1_df[\"payment_delay_days\"] = tui_val1_df[\"clear_date\"].sub(tui_val1_df[\"due_in_date\"]).dt.days.copy()\r\n",
        "tui_val1_df[\"orig_payment_window_days\"] = tui_val1_df[\"due_in_date\"].sub(tui_val1_df[\"document_create_date.1\"]).dt.days.copy()\r\n",
        "\r\n",
        "tui_val1_df_Y = tui_val1_df[\"payment_delay_days\"]\r\n",
        "tui_val1_df_X = tui_val1_df.drop(columns = [\"payment_delay_days\", \"clear_date\", \"document_create_date.1\", \"due_in_date\"]).copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G7RltTXarmmi"
      },
      "source": [
        "dfTest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E67b0cioYyU"
      },
      "source": [
        "#from datetime import date\r\n",
        "dfTest[\"Total_no_days_taken\"]=dfTest[\"due_in_date\"].sub(dfTest[\"document_create_date.1\"]).dt.days.copy()\r\n",
        "\r\n",
        "tui_test1_df = dfTest.drop(columns = [\"cust_number\", \"name_customer\"]).copy()\r\n",
        "\r\n",
        "\r\n",
        "tui_test1_df[\"total_open_amount\"] = tui_test1_df[\"total_open_amount\"].map(lambda x : round(x, 2))\r\n",
        "tui_test1_df[\"orig_payment_window_days\"] = tui_test1_df[\"due_in_date\"].sub(tui_test1_df[\"document_create_date.1\"]).dt.days.copy()\r\n",
        "\r\n",
        "tui_test1_df_X = tui_test1_df.drop(columns = [\"clear_date\", \"document_create_date.1\", \"due_in_date\"]).copy()\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zchl-4-sktIR"
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\r\n",
        "from sklearn.svm import SVR\r\n",
        "from sklearn.tree import DecisionTreeRegressor\r\n",
        "from sklearn.ensemble import RandomForestRegressor\r\n",
        "import xgboost as xgb\r\n",
        "from sklearn.datasets import load_boston\r\n",
        "from sklearn.metrics import mean_squared_error\r\n",
        "from sklearn.metrics import r2_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMY8N1I_kvVE"
      },
      "source": [
        "MSE_Score = []\r\n",
        "R2_Score = []\r\n",
        "Algorithm = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btmyk3jNkxZf"
      },
      "source": [
        "X_train = tui_df_X.copy()\r\n",
        "y_train = tui_df_Y.copy()\r\n",
        "X_test = tui_val1_df_X.copy()\r\n",
        "y_test = tui_val1_df_Y.copy()\r\n",
        "X_finaltest = tui_test1_df_X.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vn6TthpBc_BM"
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKSeWraWifpT"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkrh_7ZfdC_f"
      },
      "source": [
        "y_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B79aq65OkHfZ"
      },
      "source": [
        "y_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rEa4ExmskKkg"
      },
      "source": [
        "X_train.sort_index(inplace=True,axis=1)\r\n",
        "X_test.sort_index(inplace=True,axis=1)\r\n",
        "X_finaltest.sort_index(inplace=True,axis=1)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwDdspFUkjzs"
      },
      "source": [
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89vAIwv5kk9P"
      },
      "source": [
        "X_test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpwJtgp-tRqK"
      },
      "source": [
        "X_finaltest.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-t0yiGk5iR"
      },
      "source": [
        "### **1. Linear Regression**\r\n",
        "\r\n",
        "# Fitting Simple Linear Regression to the Training Set\r\n",
        "Algorithm.append('Linear Regression')\r\n",
        "clf = LinearRegression()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting the Test Set Results\r\n",
        "predicted = clf.predict(X_test)\r\n",
        "\r\n",
        "# Appending the Scores For Visualisation at a Later Part\r\n",
        "MSE_Score.append(mean_squared_error(y_test, predicted))\r\n",
        "R2_Score.append(r2_score(y_test, predicted))\r\n",
        "\r\n",
        "### **2. Support Vector Regression**\r\n",
        "\r\n",
        "# Fitting SVR to the Training Set\r\n",
        "Algorithm.append('Support Vector Regression')\r\n",
        "clf = SVR()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting the Test Set Results\r\n",
        "predicted = clf.predict(X_test)\r\n",
        "\r\n",
        "# Appending the Scores For Visualisation at a Later Part\r\n",
        "MSE_Score.append(mean_squared_error(y_test, predicted))\r\n",
        "R2_Score.append(r2_score(y_test, predicted))\r\n",
        "\r\n",
        "### **3. Decision Tree Regressor**\r\n",
        "\r\n",
        "# Fitting Decision Tree to the Training Set\r\n",
        "Algorithm.append('Decision Tree Regressor')\r\n",
        "clf = DecisionTreeRegressor()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting the Test Set Results\r\n",
        "predicted = clf.predict(X_test)\r\n",
        "\r\n",
        "# Appending the Scores For Visualisation at a Later Part\r\n",
        "MSE_Score.append(mean_squared_error(y_test, predicted))\r\n",
        "R2_Score.append(r2_score(y_test, predicted))\r\n",
        "\r\n",
        "### **4. Random Forest Regressor**\r\n",
        "\r\n",
        "# Fitting Random Forest Regressor Tree to the Training Set\r\n",
        "Algorithm.append('Random Forest Regressor')\r\n",
        "clf = RandomForestRegressor()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting the Test Set Results\r\n",
        "predicted = clf.predict(X_test)\r\n",
        "\r\n",
        "# Appending the Scores For Visualisation at a Later Part\r\n",
        "MSE_Score.append(mean_squared_error(y_test, predicted))\r\n",
        "R2_Score.append(r2_score(y_test, predicted))\r\n",
        "\r\n",
        "### **5. XGB Regressor**\r\n",
        "\r\n",
        "# Fitting XGBoost Regressor to the Training Set\r\n",
        "Algorithm.append('XGB Regressor')\r\n",
        "clf = xgb.XGBRegressor()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting the Test Set Results\r\n",
        "predicted = clf.predict(X_test)\r\n",
        "\r\n",
        "# Appending the Scores For Visualisation at a Later Part\r\n",
        "MSE_Score.append(mean_squared_error(y_test, predicted))\r\n",
        "R2_Score.append(r2_score(y_test, predicted))\r\n",
        "\r\n",
        "## Score Card of the Models' Performances\r\n",
        "\r\n",
        "# Just Combining the Lists into a DataFrame for a Better Visualisation\r\n",
        "Comparison = pd.DataFrame(list(zip(Algorithm, MSE_Score, R2_Score)), columns = ['Algorithm', 'MSE_Score', 'R2_Score'])\r\n",
        "\r\n",
        "# Score Card\r\n",
        "Comparison"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbWChaUXk7qF"
      },
      "source": [
        "#choosing Decision Tree Regresser\r\n",
        "X_finaltest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ozKn7p5kmz1D"
      },
      "source": [
        "# Fitting Decision Tree to the Training Set\r\n",
        "Algorithm.append('Decision Tree Regressor')\r\n",
        "clf = DecisionTreeRegressor()\r\n",
        "clf.fit(X_train, y_train)\r\n",
        "\r\n",
        "# Predicting the Test Set Results\r\n",
        "prediction = clf.predict(X_finaltest)\r\n",
        "print(len(prediction))\r\n",
        "dfTest['payment_delay_days']=prediction\r\n",
        "#dfCopyTest[\"clear_date\"]=(dfCopyTest[\"due_in_date\"])+timedelta(days=dfTest[\"payment_delay_days\"])\r\n",
        "#dfcopyTest\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8cME6-QuQ1r"
      },
      "source": [
        "dfTest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qy_9o4FNBBee"
      },
      "source": [
        "\"\"\" The different Ageing buckets will be :\r\n",
        "● 0-15 days\r\n",
        "● 16-30 days\r\n",
        "● 31-45 days\r\n",
        "● 46-60 days\r\n",
        "● Greater than 60 days\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MP9DWKp4CUL4"
      },
      "source": [
        "bins=[-60,15,30,45,60,1000]\r\n",
        "labels=[0,1,2,3,4]\r\n",
        "dfTest['bins']=pd.cut(dfTest['payment_delay_days'],bins=bins,labels=labels,include_lowest=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxOGIoyU9wAu"
      },
      "source": [
        "#import pandas as pd\r\n",
        "#dfcopyTest[\"clear_date\"]=(dfcopyTest[\"due_in_date\"])+timedelta(days=dfTest[\"payment_delay_days\"])\r\n",
        "#dfcopyTest=pd.dataFrame()\r\n",
        "\"\"\"\r\n",
        "for row in dfcopyTest.iterrows():\r\n",
        "  row[\"clear_date\"]=(rows[\"due_in_date\"])+timedelta(days=row[\"payment_delay_days\"])\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Up9yd7eD-A1Y"
      },
      "source": [
        "dfTest"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0XEL9dnD1Yl"
      },
      "source": [
        "dfTest.bins.unique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wftr-m2ETkB"
      },
      "source": [
        "dfTest.bins"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oq4LWZ8sEVph"
      },
      "source": [
        "len(dfTest.bins.isna()) # all columns are allocated bags"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
